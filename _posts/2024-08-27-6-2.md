---
layout: post
title:  "06-2 k-평균"
date:   2024-08-27 
categories: Khuda ML
---

### k-평균 알고리즘 소개

1. 무작위로 k개의 클러스터 중심을 정한다.
2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정한다.
3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경한다.
4. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복한다.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/0386ac96-77e9-4c28-85da-bcca3e49696a/image.png)

### KMeans 클래스

```python
iwget https:// bit.;y/-fruits_300-fruits_300.npy

import numpy as np
fruits= np.load('fruits_300.npy')
fruits_2d= fruits.reshape(-1,100*100)
```

k-평균 모델을 훈련하기 위해 (샘플 개수, 너비, 높이) 크기의 3차원 배열을 (샘플 개수, 너비*높이)크기를 가진 2차원 배열로 변경한다

비지도 학습이라 fit() 메서드에서 타깃 데이터 사용하지 않는다.

```python
from sklearn. cluster import KMeans 
km= KMeans(n_clusters=3,random_state=42)
km.fit(fruits_2d)
print(km.labels_)
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/b0c8fd43-5292-4ab1-9f82-dca83fcde0b5/image.png)

```python
print(np.unique(km.labels_,return_counts=True))
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/955c2f36-b139-4af3-9fa9-798ee964a358/image.png)

첫번째 클러스터(레이블0) 가 91개의 샘플을 모았고, 두 번째 클러스터(레이블1)가 98개의 샘플을 모았다. 세 번째 클러스터는 111개의 샘플을 모았다.

```python
import matplotlib.pyplot as plt
def draw_fruits(arr,ratio=1):
  n=len(arr)  #n은 샘플 개수
  #한 줄에 10개씩 이미지 그린다. 샘플 개수를 10으로 나누어 전체 행 개수 계산.
  rows=int(np.ceil(n/10))
  #행이 1개면 열의 개수는 샘플 개수이다. 그렇지 않으면 10개.
  cols=n if rows<2 else 10
  fig, axs= plt.subplots(rows,cols,figsize=(cols*ratio,rows*ratio),squeeze=False)
  for i in range (rows):
    for j in range(cols):
      if i*10+j<n:  #n개까지만 그린다.
        axs[i,j].imshow(arr[i*10+j],cmap='gray_r')
      axs[i,j].axis('off')
    plt.show()
draw_fruits(fruits[km.labels_==0])
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/3f184e65-0583-4ee2-b20b-4e673b9b980e/image.png)

**`def draw_fruits(arr, ratio=1):`**

- `draw_fruits`라는 함수를 정의. 이 함수는 이미지 배열을 입력받아, 이를 그리드 형태로 시각화한다.
- `arr`은 이미지 데이터 배열을 나타내며, `ratio`는 각 이미지의 크기를 조정하기 위한 비율을 의미한다. 기본값은 1이다.
- **`rows = int(np.ceil(n / 10))`**
    - 한 줄에 10개의 이미지를 그리도록 설정하고, 이를 기준으로 전체 행(row)의 개수를 계산.
    - `n`을 10으로 나눈 후, `np.ceil` 함수를 사용하여 올림 처리하여 행의 개수를 계산한다. 결과는 정수로 변환.
- **`cols = n if rows < 2 else 10`**
    - 만약 행이 1개라면 열의 개수는 샘플 개수 `n`과 동일하게 설정. (즉, 한 행에 모든 이미지를 표시)
    - 그렇지 않으면 열의 개수를 10으로 설정.
- **`fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False)`**
    - `plt.subplots`를 사용하여 `rows x cols` 형태의 서브플롯을 생성
    - `figsize`는 이미지 크기를 설정. `cols*ratio`와 `rows*ratio`를 사용하여 전체 이미지의 크기를 비율에 맞게 조정
    - `squeeze=False`는 서브플롯이 2차원 배열 형태로 유지되도록 한다. (만약 `squeeze=True`라면, 서브플롯이 1차원으로 축소될 수 있.)
- **`if i*10+j < n:`**
    - 현재 인덱스가 `n`(이미지 개수)보다 작은 경우에만 이미지를 그린다. (`i*10+j`는 현재의 이미지 인덱스를 계산)
- **`axs[i, j].imshow(arr[i*10+j], cmap='gray_r')`**
    - 현재 서브플롯 `axs[i, j]`에 이미지를 표시. `arr[i*10+j]`는 현재 인덱스의 이미지를 나타내며, `cmap='gray_r'`은 이미지를 역상(gray_r)으로 표시.
- **`axs[i, j].axis('off')`**
    - 현재 서브플롯의 축을 비활성화하여 축 눈금과 라벨을 제거한다.

km.labels_==0과 같이 쓰면 km.labels_배열에서 값이 0인 위치는 True, 그 외는 모두 False가 된다. 

```python
draw_fruits(fruits[km.labels_==1])
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/ad33d548-6fcf-4f61-a998-973476c594c4/image.png)

```python
draw_fruits(fruits[km.labels_==2])
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/0ac33048-43fb-438d-95dc-a986767c7c3d/image.png)

```python
draw_fruits(km.cluster_centers_.reshape(-1,100,100),ratio=3)
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/2009391e-abf1-4942-821d-18b10d2db164/image.png)

transform()메서드는 KMeans클래스의 훈련 데이터 샘플에서 클러스터 중심까지 거리로 변환해준다. 마치 StandardScaler클래스처럼 특성값 변환하는 도구로 사용할 수 있다는 의미이다.

```python
print(km.transform(fruits_2d[100:101]))
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/1d875d11-90f5-4053-9905-c82b176e4325/image.png)

하나의 샘플을 전달했기 때문에 반환된 배열은 크기가 (1, 클러스터 개수)인 2차원 배열이다. 첫번째 클러스터 (레이블0), 두번째 클러스터(레이블1) 가 각각 첫번째 원소, 두번째 원소 값이다. 세번째 클러스터까지의 거리가 3393.8로 가장 작다. 

```python
print(km.predict(fruits_2d[100:101]))
```

>>[2]

레이블2로 예측, 이 샘플은 파인애플 일 것.

```python
draw_fruits(fruits[100:101])
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/49828553-921e-43f9-a540-83516f8c84ee/image.png)

```python
print(km.n_iter_)
```

>>3

n_iter_는 K-means클러스터링 알고리즘에서 사용되는 속성으로 최종 클러스터링이 완료될 때까지 수행된 반복 횟수를 나타낸다. 

### 최적의 k찾기

군집 알고리즘에서 적절한 k값을 찾기 위한 완벽한 방법은 없다. 몇 가지 도구가 있지만 저마다 장단점이 있다. 적절한 클러스터 개수 찾기 위한 대표적인 방법이 엘보우 방법이 있다.

k-평균 알고리즘 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 잴 수 있는데 이 거리의 제곱 합을 **이너셔** 라고 부른다. 

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/cebff34e-7c1c-4bc0-8c52-ca8e6264ccd2/image.png)

꺾이는 이 지점이 마치 팔꿈치 모양이라 엘보우 방법이라 부른다.

```python
inertia=[]
for k in range(2,7):
  km= KMeans(n_clusters=k,random_state=42)
  km.fit(fruits_2d)
  inertia.append(km.inertia_)
plt.plot(range(2,7),inertia)
plt.show()
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/e67a9746-c7da-4c08-91e7-44f84ac74489/image.png)

k=3에서 그래프의 기울기가 조금 바뀌었다. 엘보우 지점보다 클러스터 개수가 많아지면 이너셔의 변화가 줄어들면서 군집효과도 줄어든다.