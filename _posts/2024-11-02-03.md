---
layout: post
title:  "10장 앙케트 분석을 위한 자연어 처리 테크닉 10"
date:   2024-11-02 
categories: Khuda 데이터분석 실무
---

전제조건 : 4개월간 모은 고객만족도 설문조사 데이터가 데이터베이스에 기록돼있음. 

여기서 다룰 데이터에는 설문조사를 한 날짜, 의견, 만족도의 결과가 들어 있다.

### 테크닉 91 데이터를 불러서 파악해 보자

```python
import pandas as pd

survey = pd.read_csv("survey.csv")
print(len(survey))
survey.head()

#결측치확인
survey.isna().sum()  

#결측치 제거
survey= survey.dropna()
survey.isna().sum()
```

### 테크닉 92 불필요한 문자를 제거하자

```python
survey["comment"] = survey["comment"].str.replace("\(.+?\)", "")
survey.head()
```

comment 열에서 괄호()로 둘러싸인 모든 텍스트를 제거한다. 

괄호 사이의 +? 는 1문자 이상이라는 의미이다. 

### 테크닉 93 문자 수를 세어 히스토그램으로 표시해 보자

```python
survey["length"] = survey["comment"].str.len()
survey.head()

#히스토그램 그리기
import matplotlib.pyplot as plt
%matplotlib inline
plt.hist(survey["length"])
```

### 테크닉 94 형태소 분석으로 문장을 분해해 보자

```python
from konlpy.tag import Twitter

twt = Twitter()
text = "형태소분석으로 문장을 분해해보자" 
tagging = twt.pos(text)
tagging 
```

`konlpy` 라이브러리에서 `Twitter` 형태소 분석기를 불러온다. 이 분석기는 한국어 텍스트를 형태소 단위로 분석하는 데 사용된다. (참고: `Twitter` 클래스는 최신 버전에서 `Okt`로 변경되었다.)

pos()함수는 형태소와 품사를 쌍(tuple)으로 반환한다.

```python
words = twt.pos(text)
words_arr = []
for i in words:
    if i == 'EOS': continue #현재요소 i가 EOS인 경우 이를 건너뛰고 다음 반복으로 넘어간다. 
    word_tmp = i[0] #i튜플의 첫 번째 요소만 저장
    words_arr.append(word_tmp) #추출한 형태소를 리스트에 추가

words_arr
```

![Untitled](/assets/HW1/jj6.png)

### 테크닉 95 형태소 분석으로 문장에서 ‘동사’, ‘명사’를 추출해 보자

```python
text = "형태소분석으로 문장을 분해해보자"
words_arr = []
parts = ["Noun", "Verb"] #관심있는 품사 목록 정의, 여기서 명사, 동사를 포함
words = twt.pos(text)
words_arr = []

for i in words: 
    if i == 'EOS' or i == '': continue
    word_tmp = i[0]
    part = i[1]

    if not (part in parts): continue #포함되지 않은 경우, 현재 반복 건너뛴다.
    words_arr.append(word_tmp)

words_arr

```

테크닉 94 와의 차이점은 품사가 명사나 동사가 아닌 단어는 continue에 의해 words_arr에 저장되지 않는다는 것이다. 실행 결과를 보면 앞에서와는 달리 다섯 단어만 추출된 것을 알 수 있다. 이것으로 단어의 분할뿐만 아니라 특정 단어만 추출하는 것도 할 수 있게 됐다.

### 테크닉 96 형태소 분석으로 자주 나오는 명사를 확인해 보자

```python
all_words = []
parts = ["Noun"] #명사만 추출

for n in range(len(survey)):
    text = survey["comment"].iloc[n]
    words = twt.pos(text)
    words_arr = []
    
    for i in words:
        if i == "EOS" or i == "": continue
        word_tmp = i[0]
        part = i[1]
        
        if not (part in parts): continue
        words_arr.append(word_tmp)
    
    all_words.extend(words_arr)

print(all_words)
```

![Untitled](/assets/HW1/jj7.png)

실행시 명사만 추출되고, ‘역앞’ 이라는 단어를 찾아보면 여러 개가 존재한다. 단어별로 빈도수를 알아보기 위해 이것을 데이터 프레임에 저장하고 집계해서자주 나오는 단어 5개 표시해보자.

```python
all_words_df = pd.DataFrame({"words": all_words, "count": [1] * len(all_words)})
all_words_df = all_words_df.groupby("words").sum()
all_words_df.sort_values("count", ascending=False).head()
```

1행: 단어와 빈도수를 데이터프레임에 저장, count칼럼을 작성하고 전부 1을 대입한다.

2행: 단어마다 count를 계산

3행: 내림차순으로 처음 5행을 표시

### 테크닉 97 관계없는 단어를 제거해 보자

```python
stop_words = ["더", "수", "좀"] #불용어리스트 정의
all_words = []
parts = ["Noun"]

for n in range(len(survey)):
    text = survey["comment"].iloc[n]
    words = twt.pos(text)
    words_arr = []
    
    for i in words:
        if i == "EOS" or i == "": continue
        word_tmp = i[0]
        part = i[1]
        
        if not (part in parts): continue
        if word_tmp in stop_words: continue
        words_arr.append(word_tmp)
    
    all_words.extend(words_arr)

print(all_words)
```

테크닉 96과의 차이점은 stop_words라는 변수를 정의하고 명사가 만약 stop_words인 경우에는 words_arr에 추가되지 않게 하는 것이다. 

```python
#명사만 데이터 프레임에 저장하고 많이 나오는 단어들 표시
all_words_df = pd.DataFrame({"words": all_words, "count": [1] * len(all_words)})
all_words_df = all_words_df.groupby("words").sum()
all_words_df.sort_values("count", ascending=False).head()
```

### 테크닉 98 고객만족도와 자주 나오는 단어의 관계를 살펴보자

```python
stop_words = ["더", "수", "좀"] #단어분석에서 제외
parts = ["Noun"]
all_words = []
satisfaction = []

for n in range(len(survey)):
    text = survey["comment"].iloc[n] #n번째 텍스트 데이터를 추출
    words = twt.pos(text)
    words_arr = []
    
    for i in words:
        if i == "EOS" or i == "": continue
        word_tmp = i[0]
        part = i[1]
        
        if not (part in parts): continue
        if word_tmp in stop_words: continue #불용어 리스트에 포함된 경우 건넌뛴다.
        words_arr.append(word_tmp)#조건 만족하면 추가
        satisfaction.append(survey["satisfaction"].iloc[n])
    
    all_words.extend(words_arr)

all_words_df = pd.DataFrame({"words": all_words, "satisfaction": satisfaction, "count": [1] * len(all_words)})
all_words_df.head()
```

테크닉 97 과의 차이점은 words_arr에 단어를 추가하는 words_arr.append(word_tmp)아래에 satisfaction에 만족도를 추가하는 처리가 추가됨. 

```python
words_satisfaction = all_words_df.groupby("words").mean()["satisfaction"]
words_count = all_words_df.groupby("words").sum()["count"]
words_df = pd.concat([words_satisfaction, words_count], axis=1)
words_df.head()

```

1행: 각 단어의 satisfaction평균값 계산해 저장

2행: words열을 기준으로 그룹화하고 각 단어의 count합계 계산해 저장

```python
#count3이상인 데이터만 뽑아서 고객만족도를 내림차순 및 오름차순으로 5개씩 나열
words_df = words_df.loc[words_df["count"] >= 3]
words_df.sort_values("satisfaction", ascending=False).head()
words_df.sort_values("satisfaction").head()
```

![Untitled](/assets/HW1/jj8.png)

### 테크닉 99 의견을 특징으로 표현해 보자

```python
parts = ["Noun"]
all_words_df = pd.DataFrame()
satisfaction = []

for n in range(len(survey)):
    text = survey["comment"].iloc[n]
    words = twt.pos(text)
    words_df = pd.DataFrame()
    
    for i in words:
        if i == "EOS" or i == "": continue
        word_tmp = i[0]
        part = i[1]
        
        if not (part in parts): continue
        words_df[word_tmp] = [1]
    
    all_words_df = pd.concat([all_words_df, words_df], ignore_index=True)

all_words_df.head()

```

words_df[word_tmp]=[1]로 단어가 포함돼 있으면 숫자 1을 대입한다. 이것을 모든 의견마다 진행하고 concat으로 결합한다. 

```python
all_words_df = all_words_df.fillna(0)
all_words_df.head()
```

코드 실행하면 결측치가 0으로 채워짐.

### 테크닉 100 비슷한 설문지를 찾아보자

```python
print(survey["comment"].iloc[2])
target_text = all_words_df.iloc[2]
print(target_text)
```

‘육아 지원이 좋다’ 라는 의견을 타깃으로 하고, 테크닉 99에서 all_words_df의 인덱스 번호가 2인 데이터를 가져왔다. 표시 결과를 살펴보면 1/0의 플래그 리스트가 표시된다.

![Untitled](/assets/HW1/jj9.png)

```python
#유사도 검색 진행
import numpy as np

cos_sim = []
for i in range(len(all_words_df)):
    cos_text = all_words_df.iloc[i]
    #target_text와 cos_text간의 코사인 유사도를 계산 
    cos = np.dot(target_text, cos_text) / (np.linalg.norm(target_text) * np.linalg.norm(cos_text))
    cos_sim.append(cos)

all_words_df["cos_sim"] = cos_sim
all_words_df.sort_values("cos_sim", ascending=False).head()
```

![Untitled](/assets/HW1/jj10.png)

- `np.dot(target_text, cos_text)`: 두 벡터의 내적을 구한다.
- `np.linalg.norm(target_text)`: `target_text`의 벡터 크기를 계산
- `np.linalg.norm(cos_text)`: `cos_text`의 벡터 크기를 계산
- 이 계산은 코사인 유사도를 측정하여 두 벡터 간의 유사성을 수치로 나타낸다.

→ 인덱스 2는 타깃 의견 자신이므로 유사도 1. 따라서 인덱스 15, 24인 의견이 가장 높은 유사도 갖는다.

```python
#상위 의견 표시
print(survey["comment"].iloc[2])
print(survey["comment"].iloc[15])
print(survey["comment"].iloc[24])
```