# 04-1 로지스틱 회귀

머신러닝으로 럭키백의 생선이 어떤 타깃에 속하는지 확률을 구해보자. 클래스 확률에 대해 배워보자.

```python
import pandas as pd
fish=pd.read_csv('https://bit.ly/fish_csv')
fish.head()
```

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/63a49154-b07a-4209-91ab-338d9a9d1b7f/Untitled.png)

```python
print(pd.unique(fish['Species']))#어떤 종류의 생선이 있는지 Species열에서 추출
```

>>`['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']`

---

- unique()함수는 배열이나 시퀀스에서 고유한 값들만 추출하여 반환하는 함수. 데이터 분석과 처리에서 중복된 값을 제거하고 고유한 값을 얻고자 할 때 유용.

```python
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
print(fish_input[:5])
```

>>

[[242.      25.4     30.      11.52     4.02  ]
[290.      26.3     31.2     12.48     4.3056]
[340.      26.5     31.1     12.3778   4.6961]
[363.      29.      33.5     12.73     4.4555]
[430.      29.      34.      12.444    5.134 ]]

Species열을 타깃으로, 나머지 열은 입력데이터로 사용.  to_numpy()메서드로 넘파이 배열로 바꾸어 fish_input에 저장.

```python
fish_target=fish['Species'].to_numpy()#타깃 데이터 만들
```

```python
from sklearn.model_selection import train_test_split
train_input,test_input,train_target,test_target=train_test_split(fish_input,fish_target,random_state=42)
```

```python
from sklearn.preprocessing import StandardScaler
ss=StandardScaler()
ss.fit(train_input)
train_scaled=ss.transform(train_input)
test_scaled=ss.transform(test_input)
```

k-최근접 이웃 분류기의 확률 예측해보자

```python
from sklearn.neighbors import KNeighborsClassifier
kn=KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled,train_target)
print(kn.score(train_scaled,train_target))
print(kn.score(test_scaled,test_target))
```

>>0.8907563025210085
>>0.85

2개 이상의 클래스가 포함된 문제를 다중분류라고 부른다. 

```python
print(kn.classes_)
```

>> ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] #알파벳 순으로 출력

```python
#테스트 세트의 처음 5개 샘플의 타깃값 예측
print(kn.predict(test_scaled[:5]))
```

>>['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']

```python
import numpy as np
proba= kn.predict_proba(test_scaled[:5]) #클래스별 확률값 반환
print(np.round(proba, decimals = 4))#소수점 다섯번째 자리에서 반올림해 넷째자리까지 표기
```

>> 

[[0.     0.     1.     0.     0.     0.     0.    ]
[0.     0.     0.     0.     0.     1.     0.    ]
[0.     0.     0.     1.     0.     0.     0.    ]
[0.     0.     0.6667 0.     0.3333 0.     0.    ]
[0.     0.     0.6667 0.     0.3333 0.     0.    ]]

첫번째 열이 Bream에 대한 확률, 두번째 열이 Parkki에 대한 확률

```python
distances, indexes=kn.kneighbors(test_scaled[3:4])
print(train_target[indexes]) #그인덱스에 해당하는 학습데이터 타깃값
```

>> [['Roach' 'Perch' 'Perch']]

첫번째 줄: 테스트 데이터의 네번째 샘플(인덱스 3) 선택. 이 데이터 포인트에 대해 가장 가까운 이웃들 찾는 것. 

이 샘플의 이웃은 다섯번째 클래스인 Roach 1개,(1.3 = 0.3333) 세번째 클래스인 Perch2개(2/3=0.6667). 앞에 출력한 네 번째 샘플의 클래스 확률과 같다. 

### 로지스틱 회귀

로지스틱 회귀는 회귀지만 분류모델이다. 이 알고리즘은 선형 회귀와 동일하게 선형 방정식을 학습한다. 

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/26f9d57e-d942-42cf-8c58-0390e595b295/Untitled.png)

abcde는 가중치(계수)이다. z는 어떤 값도 가능하지만 확률이 되려면 0과 1 사이 값이어야 한다. 이를 바꾸는 방법이 시그모이드함수(로지스틱 함수)를 사용하는 것이다. 

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/3ee6df2b-48a6-4699-8881-2db3750bac0e/Untitled.png)

z가 무한하게 큰 음수일 때는 함수는 0에 가까워지고, z가 무한하게 큰 양수가 될때는 1에 가까워지고, z가 0이면 0.5가 된다. 

```python
import numpy as np
import matplotlib.pyplot as plt
z=np.arange(-5,5,0.1)#-5에서 5까지 0.1 간격으로 값 생성
phi=1/(1+np.exp(-z))#시그모이드 함수 계산
plt.plot(z,phi)
plt.xlabel('z')
plt.ylabel('phi')
plt.show()

```

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/f6715ff4-1dd2-4b00-adac-1a9d8eebfa0f/Untitled.png)

### 로지스틱 회귀로 이진 분류 수행하기

이진 분류일 경우 시그모이드 함수의 출력이 0.5보다 크면 양성 클래스, 0.5보다 작으면 음성 클래스로 판단.

**불리언 인덱싱:** 넘파이 배열 True, False 값을 전달해 행을 선택. 

```python
bream_smelt_indexes=(train_target=='Bream')|(train_target=='Smelt')
train_bream_smelt=train_scaled[bream_smelt_indexes]
target_bream_smelt=train_target[bream_smelt_indexes]
```

bream_smelt_indexes : 도미와 빙어일 경우 True, 그 외는 False값이 들어가있따. 이 배열로 train_scaled, train_target배열에 불리언 인덱싱 적용하면 손쉽게 도미와 빙어 데이터만 골라낼 수 있다.

이제 로지스틱 회귀모델 훈련해보자.

```python
from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
lr.fit(train_bream_smelt,target_bream_smelt)
print(lr.predict(train_bream_smelt[:5]))
```

>>['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']

```python
print(lr.predict_proba(train_bream_smelt[:5]))
```

>>[[0.99759855 0.00240145]
[0.02735183 0.97264817]
[0.99486072 0.00513928]
[0.98584202 0.01415798]
[0.99767269 0.00232731]]

predict_proba(): 예측확률 

첫번째 열이 음성클래스(0), 두번째 열이 양성 클래스(1)에 대한 확률이다. 

```python
print(lr.classes_)
```

>>['Bream' 'Smelt']

Smelt(빙어)가 양성 클래스임을 알 수 있다.

로지스틱 회귀가 학습한 계수를 확인해보자.

```python
print(lr.coef_,lr.intercept_)
```

>> [[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/38a61a48-f20d-4a2d-920f-2929ccae5686/image.png)

train_bream_smelt의 처음 5개 샘플의 z값을 출력해보자. 

```python
decisions=lr.decision_function(train_bream_smelt[:5])
print(decisions)
```

>>[-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]

이 z값을 시그모이드 함수에 통과시키면 확률을 얻을 수 있다.:  **expit()**

decisions배열의 값을 확률로 변환해보자.

```python
from scipy.special import expit
print(expit(decisions))
```

>>[0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]

출력값을 보면 predict_proba()메서드 두번째 열의 값고 동일하다. 즉, decision_function()메서드는 양성 클래스에 대한 z값을 반환한다.

잠시 정리해보자면,, 이진 분류를 위해 2개의 생선 샘플을 골라냈고 이를 사용해 **로지스틱 회귀모델**을 훈련했다. 이진 분류일 경우 **predict_proba()**메서드는 양성 클래스에 대한 z값을 계산한다. 또 coef_속성과 intercept_속성에는 로지스틱 모델이 학습한 선형 방정식의 계수가 들어있다. 

그럼 이제 이진 분류의 경험을 바탕으로 7개의 생선을 분류하는 다중 분류 문제로 넘어가보자!

### 로지스틱 회귀로 다중 분류 수행하기

LogisticRegression클래스는 기본적으로 반복적인 알고리즘 사용한다. max_iter매개변수에서 반복 횟수를 지정하며 기본값은 100이다. 충분하게 1000으로 늘려보자. 

LogisticRegression은 기본적으로 릿지 회귀와 같이 계수의 제곱을 규제한다. 규제 제어하는 **매개변수는 C**이다. **C는 alpha와 반대로 작을수록 규제가 커진다. C의 기본값은 1**이다.  규제완화 위해 20으로 늘리자. 

```python
lr=LogisticRegression(C=20,max_iter=1000)
lr.fit(train_scaled,train_target)
print(lr.score(train_scaled,train_target))
print(lr.score(test_scaled,test_target))
```

>>0.9327731092436975
>>0.925

```python
#테스트 세트의 처음 5개 샘플 예측 출력
print(lr.predict(test_scaled[:5]))
```

>>['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']

```python
#테스트 세트의 처음 5개 샘플에 대한 예측 확률 출력해보자. 
proba= lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals = 3))
```

>>[[0.    0.014 0.841 0.    0.136 0.007 0.003]
[0.    0.003 0.044 0.    0.007 0.946 0.   ]
[0.    0.    0.034 0.935 0.015 0.016 0.   ]
[0.011 0.034 0.306 0.007 0.567 0.    0.076]
[0.    0.    0.904 0.002 0.089 0.002 0.001]]

5개 샘플에 대한 예측이니 5개의 행 출력, 7개 생선에 대한 확률계산이므로 7개 열 출력. 

```python
print(lr.classes_)
```

>>['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']

첫번째 샘플은 Perch를 가장 높은 확률로 예측했다. (84.1%), 두번째 샘플은 여섯번째 열인 smelt(94.6%)가 가장 높은 확률.

```python
print(lr.coef_.shape, lr.intercept_.shape)
```

>> (7, 5) (7,)

coef_배열의 열은 5개, 행은 7개, intercept_도 7개. 다중분류는 클래스마다 z값을 하나씩 계산한다. 즉 7개 계산. 

다중 분류에서 확률은 소프트맥스 함수를 사용해 7개의 z값을 확률로 변환한다.

**소프트맥스 함수**: 하나의 선형방정식의 출력값을 0~1 사이로 압축하는 것과 달리, **여러 개의 선형 방정식의 출력값을 0~1 사이로 압축**하고 전체 합이 1이 되도록 만든다. 이를 위해 지수 함수를 사용하기 때문에 **정규화된 지수 함수**라고도 부른다. 

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/9097b022-a6b2-4361-ba30-ae33d098c4ed/image.png)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/cd54d90a-3257-4aaa-8280-5e3b300c6adf/image.png)

s1에서 s7까지 모두 더하면 분자와 분모가 같아지므로 1이 된다. (확률의 합은 1이므로..)

```python
#처음 5개 샘플에 대한 z1~z7의 값 
decision=lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals = 2))
```

>>[[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
[-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
[ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
[ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
[ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]

```python
from scipy.special import softmax
proba=softmax(decision,axis=1)
print(np.round(proba, decimals = 3))
```

앞서 구한 decision배열을 softmax()함수에 전달했다.softmax()의 axis매개변수는 소흐트맥스를 계산할 축을 지정한다. axis=1로 지정해 계산. 

앞의 proba배열과 비교해보면 결과가 일치하는 것을 볼 수 있다!

마지막으로 정리해보자!

- **로지스틱 회귀:** 선형 방정식을 사용한 분류 알고리즘. 시그모이드 함수, 소프트맥스 함수를 사용해 클래스 확률을 출력할 수 있다.
- **다중 분류**: 타깃 클래스가 2개 이상인 분류 문제. 로지스틱 회귀는 다중 분류를 위해 소프트맥스 함수를 사용해 클래스 예측.
- **시그모이드 함수**: 선형 방정식의 출력을 0과 1 사이의 값으로 압축하며 이진 분류를 위해 사용.
- **소프트맥스 함수**: 다중 분류에서 여러 선형 방정식의 출력 결과를 합이 1이 되도록 만든다.