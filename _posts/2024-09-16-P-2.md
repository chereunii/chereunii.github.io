---
layout: post
title:  "2장 대리점 데이터를 가공하는 테크닉 10"
date:   2024-09-16 
categories: Khuda 데이터분석 실무
---


### 테크닉11 데이터를 읽어 들이자

```python
import pandas as pd
uriage_data= pd.read_csv("uriage.csv")
uriage_data.head()

kokyaku_data= pd.read_excel("kokyaku_daicho.xlsx")
kokyaku_data.head()
```

같은 날짜 또는 같은 이름이 포맷이나 입력 오류 등으로 다른 데이터가 되어버릴 수 있다. 데이터의 오류를 해소하고 정합성을 보장하는 것은 중요하다. 

### 테크닉12 데이터의 오류를 살펴보자

```python
#item_name추출해서 데이터 오류 확인
uriage_data["item_name"].head()
```

간단하게 표시해 데이터 오류를 확인해보고 일단 시험삼아 이대로 집계해보자.

### 테크닉13 데이터에 오류가 있는 상태로 집계해보자.

```python
uriage_data["purchase_date"]= pd.to_datetime(uriage_data["purchase_date"])
uriage_data["purchase_month"]=uriage_data["purchase_date"].dt.strftime("%y%m")
res= uriage_data.pivot_table(index="purchase_month", columns="item_name", aggfunc="size", fill_value=0)
res
```

- **`pd.to_datetime`**: `purchase_date` 컬럼을 날짜 형식으로 변환
- **`.dt.strftime("%y%m")`**: `purchase_date`에서 연도와 월을 추출하여 `purchase_month` 컬럼을 생성
- **`pivot_table`**: `purchase_month`를 행(`index`), `item_name`을 열(`columns`)로 하여, 각 항목별로 구매 횟수를 집계한 피벗 테이블을 생성`aggfunc="size"`는 그룹 내 데이터 개수를 계산하고, `fill_value=0`은 데이터가 없는 경우 0으로 채운다.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/38182b06-60ee-4cf6-a11a-1e18ef9dbb31/image.png)

데이터 오류에 의해 원래 26개 상품이 99개 상품으로 늘어남. 마찬가지로 가로축도 집계해보기

이렇게 데이터에 오류가 있는 상태로 집계 및 분석 실시하면 전혀 의미 없는 결과가 나오기 때문에 전처리의 중요성을 확인할 수 있다.

### 테크닉 14 상품명 오류를 수정하자.

이번 사례는 간단한 오류라 공백 오류를 수정하는 것만으로 해결할 수 있을것이다.

```python
print(len(pd.unique(uriage_data.item_name)))
```

>>99

원래 26개의 상품이 99개로 늘어나 있다. 이제 데이터의 오류를 수정하자.

```python
#상품명 대문자로 변환
uriage_data["item_name"]=uriage_data["item_name"].str.upper()
#공백 제거
uriage_data["item_name"]=uriage_data["item_name"].str.replace("  ","")
uriage_data["item_name"]=uriage_data["item_name"].str.replace(" ","")
#오름차순 정
uriage_data.sort_values(by=["item_name"], ascending= True)
```

```python
print(pd.unique(uriage_data["ite_name"]))
print(len(pd.unique(uriage_data["item_name"])))
```

상품명과 개수 출력됨. 26건의 상품으로 통ㄹ일되어 데이터 오류가 없어진 것을 확인할 수 있다.

### 테크닉15 금액의 결측치를 수정하자

먼저 데이터 결측치 확인하자

```python
uriage_data.isnull().any(axis=0)
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/8ce59e09-8388-48d7-ac61-88ef33c07e0d/image.png)

item_price True라 결측치 있음을 확인할 수 있다. 금액의 결측치를 수정하자.

이번 케이스는 집계 시간에 상품 단가의 변동이 없다는 전제조건이 있었기에 같은 결측치는 같은 상품의 단가를 이용하면 수정할 수 있다.

```python
#item_price가 null인 데이터 확인
flg_is_null= uriage_data["item_price"].isnull()
#null 인 item_name에 대해 최대값으로 채우기
for trg in list(uriage_data.loc[flg_is_null, "item_name"].unique()):
	price = uriage_data.loc[(~flg_is_null)& (uriage_data["item_name"] ==trg), "item_price"].max()
	#데이터 수정
	uriage_data["item_price"].loc[(flg_is_null)& (uriage_data["item_name"] ==trg)]= price
#결과 확인
uriage_data.head()

```

list()는 변수의 값을 리스트 형식으로 변환. uriage_data.loc[flg_is_null, “item_name”]에서 loc함수는 조건에 일치하는 데이터를 추출한다.

2번째 item_name은 조건과 일치하는 데이터 중에서 어떤 칼럼 가져올지 지정한다. 이렇게 결측치가 존재하는 상품명 추출.

마지막 uniqu()는 추출한 상품명에서 중복 제거한다. 

~flg_is_null에서 ~는 부정 연산자. flg_is_null == False와 같다.

이제 각 상품의 금액이 정상적으로 수정됐는지 확인하자

```python
for trg in list(uriage_data["item_name"].sort_values().unique()):
	print(trg +"의 최고가:" +str(uriage_data.loc[uriage_data["item_name"]==trg]
	["item_price"].max()) + "의 최저가:" + str(uriage_data.loc[uriage_data
	["item_name"]== trg]["item_price"].min(skipna=False)))
```

1. **`for trg in list(...)`**: `item_name`을 정렬하고, 중복을 제거한 고유 값을 리스트로 변환한 후 반복
2. **`max_price`**: 각 `item_name`별로 `item_price`의 최고가를 계산
3. **`min_price`**: 각 `item_name`별로 `item_price`의 최저가를 계산하며, `skipna=False`를 사용하여 `NaN` 값을 무시하지 않고 고려

### 테크닉16 고객 이름의 오류를 수정하자

```python
#데이터 확인
kokyaku_data["고객이름"].head()
uriate_data["customer_name"].head()
```

데이터에 고객 정보, 매출 이력의 고객 이름 비교해보면 공백 여부가 다름.

이대로 매출 이력과 고객 정보 결합하면 정상적으로 결합되지 않음

```python
#공백제거
kokyaku_data["고객이름"] = kokyaku_data["고객이름"].str.replace("  ", "")
kokyaku_data["고객이름"] = kokyaku_data["고객이름"].str.replace(" ", "")
kokyaku_dat["고객이름"].head()
```

### 테크닉17 날짜 오류를 수정하자

데이터에 날짜 형식이 아닌 숫자가 있을 수 있다. 이 날짜를 동일한 포맷으로 통일해보자.

```python
#숫자인지 아닌지 판별
flg_is_serial = kokyaku_data["등록일"].astype("str").str.isdigit()
#숫자로만 이루어진 행들 더해 변
flg_is_serial.sum()
```

>>22

22개의 숫자 데이터가 있음을 알 수 있다.

```python
#숫자를 날짜로 변환
fromSerial= pd.to_timedelta(kokyaku_data.loc[flg_is_serial, "등록일"].astype("float"), unit="D") +pd.to_datetime("1900/01/01")
fromSerial

#날짜로 변환된 데이터 서식 통일
fromString= pd.to_datetime(kokyaku_data.loc[~flg_is_erial, "등록일"])
fromString
```

`unit="D"`는 숫자를 "일(day)" 단위로 변환하라는 의미
`loc`는 Pandas DataFrame에서 특정 행이나 열을 선택할 때 사용하는 메서드로, 라벨(label)이나 조건에 기반한 데이터를 선택할 수 있다. `loc`는 인덱스나 열 이름을 기반으로 데이터를 가져오고 수정할 때 사용된다.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/52e3d98d-6f80-4767-80c4-82a73c5155e9/image.png)

```python
#날짜로 수정한 데이터와 서식을 변경한 데이터 결합
kokyaky_data["등록일"] =pd.concat([fromSerial, fromString])
kokyaku_data
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/0226049c-0e24-432f-a909-c7504f549ae1/image.png)

```python
#등록일을 등록연월 형식으로 변환 
kokyaku_data["등록연월"] =kokyaku_data["등록일"].dt.strftime("%Y%m")
#등록연월별로 고객이름을 그룹화하고 카운트
rslt= kokyaku_data.groupby("등록연월").count()["고객이름"]

print(rslt)
print(len(kokyaku_data))
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/ff1dff23-d921-4ea2-9989-69c85cb4a45b/image.png)

```python
#숫자 항목 유무
flg_is_serial = kokyaku_data["등록일"].astype("str").str.isdigit()
flg_is_serial.sum()
```

>>0

처음에 22개였던 결과가 0개가 되어 모든 숫자 데이터가 날짜로 수정된 것을 확인할 수 있다.

### 테크닉18 고객 이름을 키로 두 개의 데이터를 결합(조인)하자

```python
join_data=pd.merge(uriage_data, kokyaku_data, left=on="customer_name", right_on="고객이름", how="left")
join_data= join_data.dfrop("customer_name", axis=1)
join_data
```

`uriage_data`와 `kokyaku_data`를 `customer_name`과 `고객이름` 열을 기준으로 병합하고, 병합 후에는 `customer_name` 열을 삭제한다.

### 테크닉19 정제한 데이터를 덤프하자

깨끗해진 데이터를 파일로 출력(덤프)해두고 분석할 때 출력한 파일을 다시 읽어 들이면 데이터 정제를 다시할 필요가 없다. 

```python
dump_data=join[["purchase_data", "purchase_month", "item_name", "item_price", "고객이름", "지역", "등록일"]]
dump_data #저장
#csv파일로 출력
dump_data.to_csv("dump_data.csv", index=False)
```

### 테크닉20 데이터를 집계하자

```python
import_data=pd.read_csv("dump_data.csv")
import_data

#purchase_month를 세로축으로해서 상품별로 집계
byItem= import_data.pivot_table(index="purchase_month", columns="item_name", aggfunc="size", fill_value=0)
byItem
```

**`aggfunc='size'`**: 그룹별 데이터의 개수를 셉니다 (빈도수).

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/be51266a-baea-4d21-a836-ae3d46717a67/image.png)

구입 연월, 상품 집계 결과

```python
#purchase_month세로축 지정, 매출금액, 고객, 지역 집계
byPrice=import_data.pivot_table(index="purchase_month", columns="item_name", values="item_price", aggunc="sum", fill_valu=0)
byPrice
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/c7fc4c67-cb7d-4ad6-bd56-a6ba02936394/image.png)

구입 연월, 매출 금액 집계 결과

또 구입 연월, 고객 이름별 구입 수 집계, 구입 연월, 지역별 판매 수 집계, 집계 기간 내 이탈 고객 결과를 확인해본다.