---
layout: post
title:  "Chapter 01 나의 첫 머신러닝"
date:   2024-07-21 15:28:48 +0900
categories: Khuda ML
---

# 01-1 인공지능과 머신러닝, 딥러닝

### 인공지능이란

인공지능은 사람처럼 학습하고 추론할 수 있는 지능을 가진 컴퓨터 시스템을 만드는 기술이다. 

- 인공일반지능(Artificial General Inteligence, AGI) : 인간과 유사한 수준의 지능을 가진 인공지능. 현재 실현되지 않았고 영화나 소설속에서 자주 등장
- 강인공지능(Strong AI) : AGI와 비슷한 개념, 인간의 지능을 완전히 모방할 수 있고 자의식을 가지고 감정, 자기인식 가짐. 실현 가능성에 대해 논란있음.
- 약인공지능(Weak AI): 특정 작업이나 문제 해결에 특화된 인공지능, 현재 우리가 사용하고 있는 대부분의 인공지능은 약인공지능에 해당. 애플의 시리, 구글의 구글 어시스턴트, 아마존의 알렉사, 알파고 등. 음성 비서, 자율 주행 자동차, 음악 추천 등.

### 머신러닝이란

머신러닝이란 규칙을 일일이 프로그래밍하지 않아도 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야이다. 머신러닝은 통계학과 깊은 관련이 있다. 컴퓨터 과학 분야의 대표적인 머신러닝 라이브러리는 사이킷런(scikit-learn)이다. 이 오픈소스 라이브러리의 발전 덕분에 머신러닝 분야는 폭발적으로 성장했다. 

### 딥러닝이란

많은 머신러닝 알고리즘 중에 인공 신경망을 기반으로 한 방법들을 통칭하여 딥러닝이라고 부른다. LeNet-5, AlexNet처럼 인공 신경망이 이전과 다르게 놀라운 성능을 달성하게 된 원동력으로 크게 세 가지를 꼽을 수 있다.: 복잡한 알고리즘을 훈련할 수 있는 풍부한 데이터와 컴퓨터 성능의 향상, 혁신적인 알고리즘 개발. 

2015년 구글은 텐서플로를, 2018년 페이스북은 파이토치 딥러닝 라이브러리를 오픈소스로 발표했다.  이들의 공통점은 인공 신경망 알고리즘을 전문으로 다루고 있다는 것과 모두 사용하기 쉬운 파이썬 API를 제공한다는 점이다. 

# 01-2 코랩과 주피터 노트북

### 구글코랩

웹 브라우저에서 무료로 파이썬 프로그램을 테스트하고 저장할 수 있는 서비스, 클라우드 기반의 주피터 노트북 개발 환경. 

- 셀: 코랩에서 실행할 수 있는 최소 단위, 텍스트 셀에서는 HTML과 마크다운을 혼용해서 사용할 수 있다.
- 코랩의 상단 툴바

![Untitled](/assets/HW1/g1.png)

텍스트 셀의 수정을 끝내려면 ESC키 누르기.  그럼 다음 그림과 같이 셀에 적용할 수 있는 기능이 아이콘으로 표시된다.

![Untitled](/assets/HW1/g2.png)

![Untitled](/assets/HW1/g3.png)

- 코드셀: 노트북의 세 번째 셀이 코드 셀.

### 노트북

코랩은 구글이 대화식 프로그래밍 환경이 주피터를 커스터마이징한 것이다. 커렙의 프로그램 작성 단위이며 일반 프로그램 파일과 달리 대화식으로 프로그램을 만들 수 있기 때문에 데이터 분석이나 교육에 매우 적합하다. 노트북에는 코드, 코드의 실행 결과, 문서를 모두 저장하여 보관할 수 있다. 

# 01-3 마켓과 머신러닝

### 생선 분류 문제

생선들을 분류하는 프로그램을 만들어 보자.

**도미 데이터**

```python
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]
```

```python
import matplotlib.pyplot as plt

plt.scatter(bream_length, bream_weight) #산점도 함수 그리기
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

```

![Untitled](/assets/HW1/g4.png)

맷플롯립(matplotlib)은 파이썬에서 과학계산용 그래프를 그리는 대표적인 패키지. 

**빙어 데이터**

```python
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
```

```python
plt.scatter(smelt_length, smelt_weight)
plt.scatter(bream_length, bream_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![Untitled](/assets/HW1/g5.png)

도미와 빙어 데이터 모두 준비완료. 이제 두 데이터를 스스로 구분하기 위한 첫번째 머신러닝 프로그램을 만들어보자!

### 첫 번째 머신러닝 프로그램

k-Nearest Neighbors(k-최근접 이웃) 알고리즘을 사용해 도미와 빙어 데이터 구분해보자. 

k-최근접 이웃 알고리즘을 사용하기 전에 앞에서 준비했던 도미와 빙어 데이터를 하나의 데이터로 합치기

```python
length= bream_length + smelt_length
weight = bream_weight + smelt_weight  

fish_data = [[l,w] for l,w in zip(  length, weight)]

print(fish_data)
```

zip()함수는 나열된 리스트에서 원소를 하나씩 꺼내주는 일을 한다. 이렇게 하나씩 꺼낸 데이터에 동일한 작업을 계속 반복하는 일을 해 주는 게 for 반복문.

for문은 zip()함수로 length와 weight리스트에서 원소를 하나씩 꺼내어 l과 w에 할당한다. 그러면 [l, w]가 하나의 원소로 구성된 리스트가 만들어진다. 


>>[[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]]

다음은 정답 데이터를 만들자!

첫 번째 생선은 도미이고 두 번째 생선도 도미 라는 식으로 어떤 생선인지 답을 만드는 것이다.  도미를 1로, 빙어는 0으로 표현해보자. 도미는 35번, 빙어는 14번 나옴. 

```python
fish_target= [1]*35 +[0]*14
print(fish_target)
```

>> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

```python
from sklearn.neighbors import KNeighborsClassifier

kn =KNeighborsClassifier()
kn.fit(fish_data, fish_target)
kn.score(fish_data, fish_target)
```

>>1.0 #모델 정확도 100%

fit() 메서드는 주어지 데이터로 알고리즘을 훈련시킨 뒤 훈련한다.

이제 객체kn이 얼마나 잘 훈련되었는지 평가해보자. 

사이킷런에서 모델을 평가하는 매서드는 score()메서드이다. 

**k-최근접 이웃 알고리즘**

```python
kn.predict([[30,600]])  
```

>> array([1])
predict()매서드는 새로운 데이터의 정답을 예측한다. 

```python
kn49=KNeighborsClassifier(n_neighbors=49) #참고 데이터를 49개로 한 kn49모델
kn49.fit(fish_data, fish_target)
kn49.score(fish_data, fish_target)
```

>>0.7142857142857146 # = 35/49

문

```python
#K-최근접 이웃 분류기 생성
kn = KNeighborsClassifier()
#주어진 데이터와 타깃을 사용해 분류기를 학습시킴
kn.fit(fish_data, fish_target)

#n을 5부터 49까지의 값으로 설정하여 반복한다. k-최근접 알고리즘의 이웃 개수를 의미. 
for n in range(5, 50):
    # k-최근접 이웃 개수 설정
    kn.n_neighbors = n
    # 점수 계산
    score = kn.score(fish_data, fish_target)
    # 100% 정확도에 미치지 못하는 이웃 개수 출력
    if score < 1: #모델의 정확도가 1미만일 경우 조건 만족
        print(n, score)
        break
```

정확도가 1(100%) 미만이어야 조건을 만족하는 이유는 100% 정확도를 달성하지 못하는 가장 작은 이웃의 개수를 찾기 위해서이다. 

1. 모델의 과적합 방지: 매우 높은 정확도를 보이지만 새로운 데이터에 대해서는 일반화하지 못해 성능이 떨어질 수 있다. 
2. 적절한 이웃의 개수 찾기: 이 코드는 100% 정확도에 도달하지 않는 가장 작은 이웃의 개수를 찾아 내 k값을 선택하려는 것이다. 너무 작은 k값은 모델이 데이터의 노이즈에 민감해질 수 있고, 너무 큰 값은 모델이 과적합 될 수 있다. 
3. 모델의 유연성 확보: 100%정확도는 학습 데이터에만 완벽하게 맞춘다는 의미로 이는 학습 데이터의 패턴을 지나치게 따른다는 것을 의미. 새로운 데이터에 대해 모델이 잘 작동하지 않을 가능성을 높임.
