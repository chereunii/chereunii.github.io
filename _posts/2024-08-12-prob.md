# 04-2 확률적 경사 하강법

### 점진적 학습

점진적 학습(온라인 학습): 모델이 데이터를 한꺼번에 학습하는 것이 아니라 작은 데이터 조각들을 순차적으로 학습하면서 점진적으로 성능을 향상기키는 기계 학습 방법. 

대표적인 점진적 학습 알고리즘은 확률적 경사 하강법이 있다.

- **확률적 경사 하강법(Stochastic Gradient Descent, SGD**): 기계 학습에서 사용되는 최적화 알고리즘의 하나로 모델의 손실 함수를 최소화하기 위해 사용됨. 경사하강법의 변형된 형태,
    
    훈련세트에서 랜덤하게 하나의 샘플을 선택해 가파른 경사를 조금 내려간다. 
    
- **경사 하강법**: 손실 함수의 기울기를 계산해 그 기울기의 반대 방향으로 파라미터를 조정함으로써 손실을 최소화하는 방법이다.  산에서 가장 가파른 길을 찾아 내려오지만 조금씩 내려와야함.
- **에포크:** 훈련 세트를 한 번 모두 사용하는 과정.
- **미니배치 경사 하강법**: 1개씩 말고 무작위로 몇 개의 샘플을 선택해 경사 하강법을 수행하는 방식.
- **배치 경사 하강법:** 한 번 경사로를 따라 이동하기 위해 전체 샘플을 사용. 전체 데이터를 사용하기에 가장 안정적인 방법. 하지만 데이터 너무 많아 한번에 전체 데이터를 모두 읽을 수 없을 수도 있음.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/3cb81d42-1553-47cc-9bf3-c7ea3da1718e/image.png)

- 신경망 알고리즘은 확률적 경사 하강법을 꼭 사용한다. 신경망은 일반적으로 많은 데이터를 사용하기에 한 번에 모든 데이터를 사용하기 어렵다. 또 모델이 복잡해 수학적인 방법으로 해답을 얻기 어렵다. 신경망 모델이 확률적 경사 하강법이나 미니배ㅣ 경사 하강법을 사용한다.
- 손실함수: 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지 측정하는 기준. 손실함수는 미분 가능해야한다.(연속적이어야함)
- 로지스틱 손실 함수(이진 크로스엔트로피 손실함수)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/4613d240-4447-408d-b90f-d0e0170cb46a/image.png)

양성 클래스(타깃 = 1) 일때 손실은 -log(예측확률)로 계산. 확률이 1에서 멀어질수록 손실은 아주 큰 양수가 됨. 음성클래스(타깃=0)일때 손실은 -log(1-예측확률)로 계산. 예측확률 0에서 멀어질수록 손실은 아주 큰 양수가된다. 

### SGDClassifier

```python
import pandas as pd
fish=pd.read_csv('https://bit.ly/fish_csv')
#Species열은 타깃 데이터, 나머지는 입력 데이터
fish_input= fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target=fish['Species'].to_numpy()
```

```python
#훈련/ 테스트 세트로 나누기
from sklearn.model_selection import train_test_split
train_input,test_input,train_target,test_target=train_test_split(fish_input,fish_target,random_state=42)
#훈련세트와 테스트 세트의 특성을 표준화 전처리. 
#꼭 훈련 세트에서 학습한 통계값으로 테스트 세트도 변환해야함. 
from sklearn.preprocessing import StandardScaler
ss=StandardScaler()
ss.fit(train_input)
train_scaled=ss.transform(train_input)
test_scaled=ss.transform(test_input)
```

여기까지는 이전까지 동일. 이제 경사하강법을 제공하는 SGDClassifier 사용해보자.

```python
from sklearn.linear_model import SGDClassifier

sc=SGDClassifier(loss='log_loss',max_iter=10,random_state=42)
sc.fit(train_scaled,train_target)
print(sc.score(train_scaled,train_target))
print(sc.score(test_scaled,test_target))
```

>>0.773109243697479
>>0.775

- `loss='log_loss'`: 손실 함수로 로지스틱 손실(Log Loss, 이진 교차 엔트로피)을 사용. 이는 이진 분류 문제에서 자주 사용되는 손실 함수다.
- `max_iter=10`: 모델을 학습시키는 동안 최대 10번의 반복(epoch)을 수행
- `random_state=42`: 결과의 재현성을 위해 난수 생성기의 시드를 설. 같은 데이터를 사용할 때 동일한 결과를 얻을 수 있다.

출력된 결과를 보면, 훈련/테스트 세트의 정확도가 낮을것을 볼 수 있다. 반복 횟수 10번이 부족한 것으로 보인다.

```python
sc.partial_fit(train_scaled,train_target)
print(sc.score(train_scaled,train_target))
print(sc.score(test_scaled,test_target))
```

>>0.8151260504201681
>>0.85

객체 다시 만들지 않고 모델을 이어서 훈련할 때 partial_fit()메서드 사용. 

결과를 보니 정확도가 향상되었다. 여러 에포크에 더 훈련해보자. 

### 에포크와 과대/과소 적합

에포크가 적으면 모델이 훈련세트를 덜 학습한다. 에포크 횟수가 충분히 많으면 훈련 세트를 완전히 학습할 것이다. 

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/626f990a-7813-4ed4-8e75-f3be58fbc195/image.png)

과대적합이 시작하기 전에 훈련을 멈추는 것을 **조기종료**라고 한다. 

```python
import numpy as np
sc=SGDClassifier(loss='log_loss',random_state=42)
train_score=[]
test_score=[]
classes=np.unique(train_target)#7개의 생선목록 만들기

#300번의 에포크 동안 훈련을 반복 진행. 
for _ in range(0,300):
    sc.partial_fit(train_scaled,train_target,classes=classes)
train_score.append(sc.score(train_scaled,train_target))
test_score.append(sc.score(test_scaled,test_target))
```

```python
import matplotlib.pyplot as plt
plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/0d58e895-d88e-426e-bcfb-e7d0f9c99ac9/image.png)

백 번째 에포크 이후에는 훈련 세트와 테스트의 점수가 조금씩 벌어지고 있다. 초기에는 과소적합되어 점수들이 낮음. 이 모델의 경우 백 번째 에포크가 적잘한 반복 횟수로 보인다. 

```python
#반복횟수 100에 맞추고 모델 다시 훈련해보기
sc=SGDClassifier(loss='log_loss',max_iter=100,tol=None,random_state=42)
sc.fit(train_scaled,train_target)
print(sc.score(train_scaled,train_target))
print(sc.score(test_scaled,test_target))
```

>>0.957983193277311
>>0.925

tol매개변수를 None으로 지정하여 자동으로 멈추지 않고 max_iter=100만큼 무조건 반복하도록 하였다. 

최종점수가 좋은 것을 확인할 수 있다. 

힌지손실(hinge loss)은 서포트 벡터 머신이라 불리는 또 다른 머신러닝 알고리즘을 위한 손실함수이다.

```python
#힌지 손실 사용해 모델 훈련
sc=SGDClassifier(loss='hinge',max_iter=100,tol=None,random_state=42)
sc.fit(train_scaled,train_target)
print(sc.score(train_scaled,train_target))
print(sc.score(test_scaled,test_target))
```

>>0.9495798319327731
>> 0.925

정리

- **확률적 경사 하강법**: 훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘.

 샘플을 하나씩 사용하지 않고 여러개 사용: **미니배치 경사 하강법**. 한 번에 전체 샘플 사용: **배치 경사 하강법**

- **손실 함수**: 확률적 경사 하강법이 최적화할 대상.  이진 분류에는 **로지스틱 회귀(or 이진 크로스엔트로피) 손실함수**를, 다중 분류에는 **크로스엔트로피 손실 함수** 사용. 회귀문제에는 **평균 제곱 오차 손실 함수**를 사용.
- **에포크**: 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번 반복을 의미. 일반적으로 경사 하강법 알고리즘은 수십~수백번의 에포크를 반복한다.