# 02-1 훈련 세트와 테스트 세트

### 지도학습과 비지도 학습

지도학습 알고리즘은 훈련하기 위한 데이터와 정답이 필요하다. 지도학습 알고리즘은 입력(데이터)과 타깃(정답)으로 이루어진 훈련데이터가 필요하다. 저번 챕터에서 말했듯 입력으로 사용된 길이와 무게를 특성이라고 함. 

### 훈련 세트와 테스트 세트

시험을 보기 전에 출제될 시험 문제와 정답을 미리 알려주고 시험을 본다면 100점 맞을 수 있다. 머신러닝도 이처럼 도미와 빙어의 데이터와 타깃을 주고 훈련한 다음, 같은 데이터로 테스트한다면 모두 맞히는 것이 당연하다. 연습문제와 시험 문제가 달라야 올바르게 학생의 능력을 평가할 수 있듯 머신러닝 알고리즘의 성능을 제대로 평가하려면 훈련 데이터와 평가에 사용할 데이터가 각각 달라야 한다. 

평가를 위해 또 다른 데이터를 준비하거나 이미 준비된 데이터 중에서 일부를 떼어 내어 활용하는 것이다. 일반적으로 후자의 경우가 많다. 평가에 사용하는 데이터를 테스트 세트, 훈련에 사용되는 데이터를 훈련 세트라고 부른다. 

```python
fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 
                10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 
                7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
```

생선의 길이와 무게를 하나의 리스트로 담은 2차원 리스트 만들기

```python
fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)]  
fish_target= [1]*35 + [0]*14
```

하나의 생선 데이터를 **샘플**이라고 부른다. 도미와 빙어는 각각 35, 14마리 있으므로 전체 데이터는 49개의 샘플이 있다.사용하는 특성은 길이와 무게 2개.

이 데이터의 처음 35개를 훈련 세트로, 나머지 14개를 테스트 세트로 사용해보자.

```python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
```

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/f0647fed-1fe1-4e6f-9595-2ed68421fb7b/165071d1-a417-43be-80f1-d6dd2ec26f48.png)

슬라이싱 연산으로 인덱스 0~34까지 처음 35개 샘플을 훈련 세트로 선택했고, 인덱스 35~48까지인 나머지 14개 샘플을 테스트 세트로 선택했다. 그런데 정확도가 0.0이 나왔다.  이유는?

도미와 빙어를 섞지 않고 훈련 세트와 테스트 세트로 나눔.. 너무 당연하잖아..

아무튼 이런 것을 **샘플링 편향**이라고 부른다. 

### 넘파이

넘파이는 파이썬의 대표적인 배열 라이브러리이다. 이는 고차원의 배열을 손쉽게 만들고 조작할 수 있는 간편한 도구를 많이 제공한다. 

```python
import numpy as np
input_arr = np.array(fish_data)
target_arr = np.array(fish_target)
print(input_arr)
```

>>

[[  25.4  242. ]
[  26.3  290. ]
[  26.5  340. ]
[  29.   363. ]
[  29.   430. ]
[  29.7  450. ]
[  29.7  500. ]
[  30.   390. ]
[  30.   450. ] …

49개의 행과 2개의 열 출력.

```python
print(input_arr.shape)# 이 명령을 사용하면 (샘플 수, 특성 수)를 출력한다.
```

이제 생선 데이터를 넘파이 배열로 준비했으므로 이 배열에서 랜덤하게 샘플을 선택해 훈련 세트와 테스트 세트로 만들 차례이다. 주의 점은 input_arr와 target_arr에서 같은 위치는 함께 선택되어야 한다. 

넘파이 **arange()**함수를 이용해 0부터 48까지 1씩 증가하는 인덱스를 간단히 만들 수 있다. 그 다음 이 인덱스를 랜덤하게 섞는다. 

넘파이에서 무작위 결과를 만드는 함수들은 실행할 때마다 다른 결과를 만든다. 일정한 결과를 얻으려면 초기에 **랜덤시드(random seed)**를 지정하면 된다. 

seed()는 넘파이에서 난수를 생성하기 위한 정수 초깃값을 지정, 초깃값이 같으면 동일한 난수를 뽑을 수 있다. 따라서 랜덤 함수의 결과를 동일하게 재현하고 싶을 때 사용한다.

```python
np.random.seed(42)
index = np.arange(49)
np.random.shuffle(index)
print(index)
```

>> 

[13 45 47 44 17 27 26 25 31 19 12  4 34  8  3  6 40 41 46 15  9 16 24 33
30  0 43 32  5 29 11 36  1 21  2 37 35 23 39 10 22 18 48 20  7 42 14 28
38]

arange()함수에 정수 N을 전달하면 ()에서부터 N-1까지 1씩 증가하는 배열을 만든다. shuffle()함수는 무작위로 섞음.

넘파이는 슬라이싱 외에 배열 인덱싱이란 기능을 제공한다. **배열 인덱싱**은 1 개의 인덱스가 아닌 여러 개의 인덱스로 한 번에 여러 개의 원소로 선택할 수 있다. 

비슷한 방식으로 리스트 대신 넘파이 배열을 인덱스로 전달할 수도 있다. 

```python
train_input = input_arr[index[:35]]
train_target = target_arr[index[:35]]
print(input_arr[13], train_input[0])
```

>>[ 32. 340.] [ 32. 340.]

첫번째 줄: index 배열의 처음 35개 요소를 가져와 input_arr의 해당 인덱스에 해당하는 요소들을 선택해 tarin_input배열에 할당한다.

정확히 일치하게 출력된 것을 볼 수 있다. 
예를 들어, `index = [13, 14, 15, ..., 47]`이고 `input_arr` 배열이 `[0, 1, 2, ..., 99]`이라고 가정하면, `train_input = input_arr[index[:35]]`는 `input_arr[13]`부터 `input_arr[47]`까지의 값을 가진다. 이 경우 `train_input[0]`은 `input_arr[13]`과 동일한 값을 가진다.

```python
import matplotlib.pyplot as plt
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(test_input[:,0], test_input[:,1])
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/daae83e3-7b1c-4b48-bae5-b653e6f34315/Untitled.png)

파란색이 훈련세트이고 주황색이 테스트 세트이다. 양쪽에 도미와 빙어가 모두 섞여있는 것을 볼 수 있다.

### 두 번째 머신러닝 프로그램

```python
kn= kn.fit(train_input, train_target)
kn.score(test_input, test_target)
```

>>1.0

정확도 100%가 나옴!

```python
kn.predict(test_input)
test_target
```

>> `array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0])`

둘다 같은거 나옴

# 02-2 데이터 전처리

# 넘파이로 데이터 준비하기

아까 그 도미와 빙어 데이터로 진행해보자. 

column_stck()함수는 전달받은 리스트를 일렬로 세운 다음 차례대로 나란히 연결한다. 예를 들면 다음과 같이 나온다. 

```python
np.column_stack(([1,2,3],[4,5,6]))
```

>>array([[1, 4],
       [2, 5],
       [3, 6]])

그럼 이제 fish_length와 fish_weight를 합치자. 처음 5개만 확인해보자. 

```python
fish_data = np.column_stack((fish_length,fish_weight))
print(fish_data[:5])
```

>>[[ 25.4 242. ]
[ 26.3 290. ]
[ 26.5 340. ]
[ 29.  363. ]
[ 29.  430. ]]

np.ones()와 np.zeros()함수를 사용해보자. np.ones함수는 지정된 형태와 데이터 타입의 배열을 모두 1로 채워 생성한다. np.zeros는 0으로 채우는 듯. 이 함수는 주로 배열을 초기화하거나 특정 값으로 채워진 배열이 필요한 경우에 사용된다. 

```python
print(np.ones(5))
```

>>[1. 1. 1. 1. 1.]

np.column_stack()와 np.concatenate()함수를 사용한다. 두 함수의 연결 방식을 그림으로 설명하면 ..이런 느낌

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/b0ae3e04-958e-487a-8f56-8e371c905a60/Untitled.png)

그럼 np.concatenate()함수를 사용해 타깃 데이터를 만들어보자. np.column_stack()처럼 연결한 리스트나 배열을 튜플로 전달해야 한다.

```python
fish_target =np.concatenate((np.ones(35),np.zeros(14)))
print(fish_target)
```

>> `[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]`

# 사이키럿으로 훈련 세트와 테스트 세트 나누기

앞에서는 넘파이 배열의 인덱스를 직접 섞어서 훈련 세트와 테스트 세트로 나누었다. 이 방법보다 더 세련된 방법을 사용해보자. 사이키럿의 model_selection 모듈 아래 있는 train_test)split()함수를 해보자. 

```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state =42)
```

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/7dad7ba8-d2bd-453f-90ef-80930d32b317/Untitled.png)

이 함수는 기본적으로 25%를 테스트 세트로 떼어 낸다. 잘 나누었는지 넘파이 배열의 shape속성으로 입력 데이터의 크기를 출력해보자.

```python
print(train_input.shape, test_input.shape)
print(train_target.shape, test_target.shape)
```

>> (36, 2) (13, 2)

>> (36,) (13,)

훈련 데이터와 테스트 데이터를 각각 36개와 13개로 나누었다. 입력 데이터는 2개의 열이 있는 2차원 배열, 타깃 데이터는 1차원 배열이다. 

넘파이 배열의 크기는 파이썬의 튜플로 표현된다. 튜플의 원소가 하나면 원소 뒤에 콤마를 추가한다. 

```python
print(test_target)
```

>> [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]

13개의 테스트 세트 중 10개가 도미(1), 3개가 빙어(0)이다. 샘플링 편향이 나타난 것을 볼 수 있다. 무작위로 데이터를 나누었을 때 샘플이 골고루 섞이지 않을 수 있다. 특히 일부 클래스의 개수가 적을 때 이런 일이 생길 수 있다. 훈련 세트와 테스트 세트에 샘플의 클래스 비율이 일정하지 않으면 모델이 일부 샘플을 올바르게 학습할 수 없다.

train_test_split()함수는 이런 문제를 간단히 해결할 수 있다. stratify매개변수에 타깃 데이터를 전달하면 클래스 비율에 맞게 데이터를 나눈다. 훈련 데이터가 작거나 특정 클래스의 샘플 개수가 적을 때 특히 유용하다. 

```python
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state =42)
print(test_target)
```

>> [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.]

빙어가 하나 늘었다.  비율이 비슷해졌다.. 이제 데이터 모두 준비완료. 

### 수상한 도미 한 마리

앞에서 준비한 데이터로 k-최근접 이웃을 훈련해보자. 

```python
import matplotlib.pyplot as plt
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25,150, marker = '^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/0227d850-7c36-4e30-92e8-493ae5972313/Untitled.png)

도미와 빙어를 분류하고 도미 데이터를 넣었는데 빙어로 나왔다.

새로운 샘플은 marker매개변수로 ‘^’로 지정하여 삼각형으로 나타냈다. 

K-최근접 이웃은 주변의 샘플 중에서 다수인 클래스를 예측으로 사용한다. KNeighborsClassifier클래스는 주어진 샘플에서 가장 가까운 이웃을 찾아주는 kneighbors()메서드를 제공한다. n_neighbors의 기본겂은 5이므로 5개의 이웃이 반환된다.

```python
distances, indexes=kn.kneighbors([[25,150]])
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25,150, marker = '^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker = 'D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/8d8f0e44-c911-4706-891f-09a39563c36b/Untitled.png)

marker=’D’로 지정하면 산점도를 마름모로 그린다. 삼각형 샘플에 가장 가까운 5개의 샘플이 초록 다이아몬드로 표시된다. 역시 예측 결과와 마찬가지로 가장 가까운 이웃에 도미가 하나뿐이다. 따라서 도미를 빙어로 예측하는 것은 무리가 아니다.

### 기준을 맞춰라

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/58f33342-0eea-48ee-bced-006c63f21688/Untitled.png)

샘플까지의 거리가 130, 92인데 딱봐도 비율이 이상한 것을 알 수 있다. 이유는 x축은 범위가 좁고(10~40), y축은 범위가 넓다(0~1000).  따라서 y축으로 조금만 멀어져도 아주 큰 값으로 계산이 된다. 이 때문에 오른쪽 위의 도미 샘플이 이웃으로 선택되지 못했던 것이다.

이를 맞추기 위해 x축의 범위를 동일하게 맞춰보자. xlim()함수를 사용한다.

```python
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25,150, marker = '^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker = 'D')
plt.xlim((0,1000))
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/2e4e58dd-f2c7-41ec-89bd-df45b9173eca/Untitled.png)

두 특성(길이, 무게)의 스케일이 다르면, 즉 데이터를 표현하는 기준이 다르면 알고리즘이 올바르게 예측할 수 없다. 그래서 특성값을 일정한 기준으로 맞춰주는 **데이터 전처리**를 진행해야한다. 

가장 널리 사용하는 방법 중 하나는 **표준점수**다. 평균- 표준편차로 계산하면 된다. 

```python
mean= np.mean(train_input, axis=0)#평균계산
std= np.std(train_input, axis=0)#표준편차 계산
print(mean, std)
```

```python
train_scaled=(train_input-mean)/std
```

**브로드캐스팅**: 작은 배열과 큰 배열 간의 연산을 효율적으로 수행하기 위해 자동으로 배열의 형태를 맞춰주는 기능이다. 

### 전처리 데이터로 모델 훈련하기

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/816bbf9b-1bf3-484e-a5fc-f2bfba3f770f/Untitled.png)

샘플도 훈련세트의 mean, std를 이용해 변환해야한다.

```python
new=([25,150]-mean)/std
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker = '^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/de500130-51b3-4909-b16e-4f3f0f625642/Untitled.png)

표준편차로 변환하기 전의 산점도와 거의 동일하며, 크게 달라진 점은 x축과 y축의 범위가 -1.5~1.5사이로 바뀌었다는 것이다. 이 데이터셋으로 k-최근접 이웃 모델을 훈련해보자.

```python
kn.fit(train_scaled, train_target)
test_scaled=(test_input-mean)/std
kn.score(test_scaled, test_target)
```

>>1.0

완벽하다. 산점도 그려도 가가운 샘플 도미로 나오는 것을 볼 수 있다.

정리하자면,, 특성의 스케일을 조정하자! 데이터 전처리의 중요성.

데이터 전처리: 머신러닝 모델에 훈련 데이터를 주입하기 전에 가공하는 단계를 말한다. 때론 데이터 전처리에 많은 시간이 소모되기도 한다.