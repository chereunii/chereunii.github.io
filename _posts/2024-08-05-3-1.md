### k-최근접 이웃 회귀

지도 알고리즘은 크게 분류와 회귀로 나뉜다. 분류는 2장에서 다뤘다. 그렇다면 회귀란 무엇인가? **회귀**는 클래스 중 하나로 분류하는 것이 아니라 **임의의 어떤 숫자를 예측하는 문제**이다. e.g. 내년도 경제 성장률 예측, 배달 도착 시간 예상

```python
import numpy as np
perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,
       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,
       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,
       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,
       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,
       44.0])
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])
```

이 데이터들로 산점도 그리기

특성데이터 x축, 타깃 데이터를 y축에 놓자. 

```python
import matplotlib.pyplot as plt 
plt.scatter(perch_length, perch_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/853ff325-4d23-48d3-9295-f4560b3591ba/Untitled.png)

matplotlib.pyplot은 그래프와 플롯 생성 위한 라이브러리인 matplotlib의 하위 모듈이다. pyplot은 MATLAB과 유사한 방식으로 그래프를 그릴 수 있도록 다양한 함수 제공한다. 

- **산점도, 선 그래프, 막대 그래프, 히스토그램 등 다양한 그래프 생성**:
    - `plt.scatter()`: 산점도 생성
    - `plt.plot()`: 선 그래프 생성
    - `plt.bar()`: 막대 그래프 생성
    - `plt.hist()`: 히스토그램 생성
- **그래프 꾸미기**:
    - `plt.xlabel()`: X축 레이블 설정
    - `plt.ylabel()`: Y축 레이블 설정
    - `plt.title()`: 그래프 제목 설정
    - `plt.legend()`: 범례 추가
- **그래프 표시**:
    - `plt.show()`: 생성한 그래프를 화면에 표시

농어 데이터를 머신러닝 모델에 사용하기 전 훈련 세트와 테스트 세트로 나누어보자.

```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target= train_test_split(perch_length, perch_weight, random_state=42)
```

perch_length: 입력 데이터로 사용될 길이 정보

perch_weight: 타겟 데이터로 사용될 무게 정보

random_state=42 :데이터 분할 시 무작위성을 유지하면서 재현 가능한 결과를 얻기 위해 난수 생성의 seed설정한다. 이 값을 설정하면 동일한 데이터를 여러 번 나누더라도 항상 같은 방식으로 나누어진다. 

사이킷런에 사용할 훈련 세트는 2차원 배열이어야 한다. perch_length가 1차원 배열이기에 이를 나눈 train_input과 test_input도 1차원 배열이다. 이를 1개의 열이 있는 2차원 배열로 바꿔야 한다. 

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/c97c6bde-9531-4121-acf1-a1686b138afb/Untitled.png)

파이썬에서 1차원 배열의 크기는 원소가 1개인 튜플로 나타낸다. [1,2,3]의 크기는 (3,)이다. 

넘파이 배열은 크기를 바꿀 수 있는 **reshape()메서드**를 제공한다. 예를들어 (4,) 배열을 (2,2) 크기로 바꿔보자.

```python
test_array= np.array([1,2,3,4])
print(test_array.shape)
```

>>(4,)    #(4,)배열인 것 확인 

```python
test_array= test_array.reshape(2,2)
print(test_array.shape)
```

>> (2,2)

자 그럼 이제 train_input과 test_input을 2차원 배열로 바꾸자. 

크기에 -1을 지정하면 나머지 원소 개수로 모두 채우라는 의미이다. 

```python
train_input= train_input.reshape(-1,1)
test_input=test_input.reshape(-1,1)
print(train_input.shape, test_input.shape)
```

train_input의 크기는 (42,) 이다. 이를 2차원 배열인 (42,1)로 바꾸려면 train_input.reshape(42,1)과 같이 사용한다. 이를 위에 적용하면, 

train_input: 배열을 2차원 배열로 변환한다는 의미. reshape(-1,1): train_input배열의 원소 수를 유지하면서 하나의 열로 구성된 2차원 배열로 변환한다.

마지막 줄에서 변환된 train_input과 test input 배열의 모양(차원)을 출력한다. 

shape속성은 배열의 차원을 나타내면 (행 수, 열 수) 형태로 반환된다. 

아무튼, 2차원 배열로 성공적으로 변환했다!

- **reshape():**배열의 크기를 바꾸는 매서드. 바꾸고자 하는 배열의 크기를 매개변수로 전달한다. 바꾸기 전후의 배열 원소 개수는 동일해야함.

### 결정계수(R²)

```python
from sklearn.neighbors import KNeighborsRegressor
knr= KNeighborsRegressor() #클래스의 인스턴스를 생성해 knr이라는 변수에 저장
knr.fit(train_input, train_target)#학습시키기
print(knr.score(test_input, test_target)) #테스트 데이터 평균
```

>> 0.992809406101064

분류는 테스트 세트에 있는 샘플을 정확하게 분류한 개수의 비율, 정확도이다. 회귀에서는 정확히 맞힌다는것이 불가능하다. 이 점수를 **결정계수, R²** 라고 부른다.  당연히 높을수록 좋음.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7af498a2-beb6-449d-a194-c4c8afcd1e0a/3c6170a7-0531-4659-ae89-238116cd0fd9/Untitled.png)

```python
from sklearn.metrics import mean_absolute_error
#테스트 세트에 대한 예측을 만든다.
test_prediction = knr.predict(test_input)
#테스트 세트에 대한 평균 절댓값 오차를 계산
mae=mean_absolute_error(test_target, test_prediction)
print(mae)
```

>>19.157142857142862 

예측이 평균적으로 19g 정도 타깃값과 다르다. 그럼 훈련세트를 사용해 평가해보는게 어떨까?

### 과대적합 vs 과소적합

```python
print(knr.score(train_input, train_target))
```

>>0.9698823289099254

보통 훈련 세트의 점수가 좀 더 높게 나온다. 그런데 여기서는 과소적합이 나타남.

만약 훈련 세트에서 점수가 굉장히 좋았는데 테스트 세트에서 점수가 굉장히 나쁘면 모델이 훈련 세트에 **과대적합**이 되었다고 한다. 즉 훈련 세트에만 잘 맞는 모델이라 테스트 세트 혹은 실전 투입했을 때 잘 작동하지 않을 것이다.  

반대로 훈련 세트보다 테스트 세트의 점수가 높거나 두 점수가 모두 너무 낮은 경우는 모델이 훈련 세트에 **과소적합** 되었다고 한다.  즉 모델이 너무 단순해 훈련 세트에 적절히 훈련되지 않은 경우이다.  또한, 훈련 세트, 테스트 세트의 크기가 매우 작을때 또한 과소적합이 일어날 수 있다. 

아무튼 우리 모델에서의 과소적합은 모델을 좀 더 복잡하게 만들며 해결해보자. 즉, 훈련 세트에 더 잘 맞게 만들자. k를 줄여보자. 이웃의 개수 줄이면 훈련 세트에 있는 국지적인 패턴에 민감해지고, 이웃의 개수를 늘리면 데이터 전반에 있는 일반적인 패턴을 따를 것이다. 

```python
#이웃의 개수를 3으로 설정한다.
knr.n_neighbors = 3
#모델을 다시 훈련한다.
knr.fit(train_input, train_target)
print(knr.score(train_input, train_target))
```

>> 0.9804899950518966

k값을 줄였더니 훈련세트의 R²점수 높아짐. 

```python
print(knr.score(test_input, test_target))
```

>> 0.9746459963987609

테스트 세트의 점수는 훈련 세트보다 낮아졌으므로 과소적합 문제를 해결한듯. 또한, 두 점수 차가 크지 않아 과대적합도 아닌것 같다. 아무튼 성공적으로 회귀모델 훈련했다!

정리해보면

- **회귀**: 임의의 수치를 예측하는 문제. 타깃값도 의임의의 수치가 된다.
- **k-최근접 이웃 회귀:** k-최근접 이웃 알고리즘을 사용해 회귀 문제를 푼다. 가장 가까운 이웃 샘플을 찾고 이 샘플들의 타깃값을 평균하여 예측으로 삼는다.
- **결정계수(R²):** 대표적인 회귀 문제의 성능 측정 도구. 1에 가까울수록 좋고, 0에 가까울수록 나쁜 모델
- **과대적합**: 훈련 세트 성능 >>> 테스트 세트 성능, 모델이 훈련 세트에 너무 집착해서 데이터에 내재된 거시적인 패턴 감지 못함.
- **과소적합**: 훈련세트, 테스트 세트 성능 모두 동일하게 낮거나 테스트 세트 성능> 훈련 세트 일때. 더 복잡한 모델을 사용해 훈련 세트에 잘 맞는 모델 만들어야 함.