---
layout: post
title:  "05-3 트리의 앙상블"
date:   2024-08-20
categories: [Khuda, week5]
---

### 정형 데이터와 비정형 데이터

정형 데이터: 어떤 구조로 되어 있다는 뜻. CSV나 데이터베이스, 엑셀에 저장하기 쉬움.

비정형 데이터: 데이터베이스나 엑셀로 표현하기 어려운 것들, 사진, 음악 등.  

정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 **알고리즘이 앙상블** 학습이다. 비정형 데이터에는 **신경망 알고리즘**을 사용한다. 

### 랜덤 포레스트

**랜덤 포레스트**: 앙상블 학습의 대표 주자 중 하나로 안정적인 성능 덕에 널리 사용되고 있다. 결정 트리를 랜덤하게 만들어 결정 트리의 숲을 만든다. 그리고 각 결정 트리의 예측을 사용해 최종 예측을 만든다. 

**부트스트랩 샘플**: 데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식을 의미한다. 

![Untitled](/assets/HW1/bb10.png)

RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근만큼의 특성을 선택한다. 

랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다. 

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
wine= pd.read_csv('https://bit.ly/wine-date')
data=wine[['alcohol','sugar','pH']].to_numpy()
target=wine['class'].to_numpy()
train_input, test_input, train_target, test_target=train_test_split(data, target, test_size=0.2, random_state=42)
```

```python
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(n_jobs=-1, random_state=42)
scores=cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

>> 0.9973541965122431 0.8905151032797809

과대적합

랜덤 포레스트는 DecisionTreeClassifier가 제공하는 criterion, max_depth, max_features, 등 제공한다. 랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것이다.

```python
rf.fit(train_input, train_target)
print(rf.feature_importances_)#특성 중요도 출력
```

>> [0.23167441 0.50039841 0.26792718]

RandomForestClassifier 자체적으로 모델을 평가하는 점수를 얻을 수 있다.  랜덤 포레스트는 훈련 세트에서 중복을 허용해 부트스트랩 샘플을 만들어 결정 트리를 훈련한다고 했다. 이 때 부ㅡ스트랩 샘플에 포함되지 않고 남는 샘플을 OOB(out of bag)라고 한다. 이 남는 샘플을 사용#해 부트스트랩 샘플로 훈련한 결정트리를 평가할 수 있다. 

```python

rf=RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
```

>> 0.8934000384837406 #교차검증에서 얻은 점수와 비슷한 결과

obb_score=False가 기본값이다. 


### 엑스트라 트리

엑스트라 트리는 랜덤 포레스트와 동일하게 결정 트리가 제공하는 대부분의 매개변수를 지원한다. 또한 전체 특성 중에 일부 특성을 랜덤하게 선택해 노드를 분할하는 데 사용한다. 하지만 결정 트리를 만들 때 부트스트랩 샘플을 사용하지 않고 전체 훈련 세트를 사용한다. 대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할해 빠른 계산 속도가 장점이다.

```python
from sklearn.ensemble import ExtraTreesClassifier
et=ExtraTreesClassifier(n_jobs=-1, random_state=42) #모델 초기화
scores=cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1) #모델 검증
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

>>0.9974503966084433 0.8887848893166506

- `n_jobs=-1`: 이 파라미터는 가능한 모든 CPU 코어를 사용하여 병렬 처리를 수행하라는 의미. 즉, 모델 훈련 및 예측을 병렬로 빠르게 수행한다.
- `random_state=42`: 이 파라미터는 난수 생성기의 seed를 설정하여 결과의 재현성을 보장한다. 같은 `random_state` 값을 사용하면 같은 데이터와 조건에서 항상 동일한 결과를 얻게 된다.
- `cross_validate`: 주어진 모델을 사용하여 교차 검증(cross-validation)을 수행하는 함수
- `return_train_score=True`: 훈련 세트에서의 점수도 반환할지 여부를 설정하는 옵션. 이 경우 훈련 세트의 점수도 반환된다.
- `n_jobs=-1`: 이 또한 모든 CPU 코어를 사용하여 교차 검증을 병렬로 수행하도록 설정한다.

```python
et.fit(train_input, train_target)
print(et.feature_importances_)
```

>>[0.20183568 0.52242907 0.27573525]


### 그레이디언트 부스팅

그레이디언트 부스팅은 깊이가 얕은 결정 트리를 사용해 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이다. 사이킷런의 GradientBoostingClassifier는 기본적으로 깊이가 3인 결정 트리를 100개 사용한다. 깊이가 얕은 결정 트리를 사용하기 때문에 과대적합에 강하고 일반화 성능이 높다. 

```python
from sklearn.ensemble import GradientBoostingClassifier
gb=GradientBoostingClassifier(random_state=42)
scores=cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

>>0.8881086892152563 0.8720430147331015

그레이디언트 부스팅은 결정 트리의 개수를 늘려도 과대적합에 강하다. 

```python
gb=GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
scores=cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

>> 0.9464595437171814 0.8780082549788999

결정 트리 개수를 5배나 늘렸는데 과대적합 억제를 확인할 수 있다. 학습률 learning_rate의 기본값은 0.1이다. 

```python
gb.fit(train_input, train_target)
print(gb.feature_importances_)
```

>>[0.15872278 0.68010884 0.16116839]

subsample은 트리 훈련에 사용할 훈련 세트의 비율을 정하는 매개변수이다. 이 매개변수의 기본값은 1.0이고 1보다 작으면 훈련 세트의 일부를 사용한다. 


### 히스토그램 기반 그레이디언트 부스팅

히스토그램 기반 그레이디언트 부스팅은 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘이다. 입력 특성을 256개 구간으로 나눈다. 한 구간을 떼어 놓고 누락된 값을 위해 사용해 따로 전처리가 필요 없다. 

```python
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
hgb= HistGradientBoostingClassifier(random_state=42)
scores=cross_validate(hgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

>> 0.9321723946453317 0.8801241948619236

과대적합을 억제하면서 그레이디언트 부스팅보다 좀 더 높은 성능을 제공한다. 

```python
hgb.fit(train_input, train_target)
print(rf.feature_importances_)
```

>> [0.23167441 0.50039841 0.26792718]

다양한 특성을 골고루 잘 평가한다.

```python
hgb.score(test_input, test_target)
```

>> `0.8723076923076923`

테스트 세트는 약 87%정확도 얻었다. 

```python
from xgboost import XGBClassifier
xgb = XGBClassifier(tree_method='hist', random_state=42)
scores=cross_validate(xgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

>> 0.9558403027491312 0.8782000074035686

```python
from lightgbm import LGBMClassifier
lgb=LGBMClassifier(random_state=42)
scores=cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

>> 0.935828414851749 0.8801251203079884

정리

- **앙상블 학습**: 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘을 말한다.
- **랜덤 포레스트**: 대표적인 결정 트리 기반의 앙상블 학습 방법. 부트스트랩 샘플을 사용하고 랜덤하게 일부 특성을 선택하여 트리 만드는 것이 특징이다.
- **엑스트라 트리**: 랜덤 포레트스와 비슷하게 결정 트리를 사용해 앙상블 모델을 만들지만 부트스트랩 샘플을 사용하지 않았다. 대신 랜덤하게 노드를 분할해 과대적합을 감소시킨다.
- **그레이디언트 부스팅**: 랜덤 포레스트나 엑스트라 트리와 달리 결정 트리를 연속적으로 추가해 손실 함수를 최소화하는 앙상블 밥업. 이런 이유로 훈련 속도가 좀 느리지만 더 좋은 성능을 기대할 수 있다. 그레이디언트 부스팅의 속도를 개선한 것이 히트로그램 기반 그레이디언트 부스팅이고 안정적이 결과와 높은 성능으로 매우 인기가 높다.
