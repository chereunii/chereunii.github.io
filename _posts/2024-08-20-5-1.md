---
layout: post
title:  "05-1 결정 트리"
date:   2024-08-20
categories: [Khuda, week5]
---


### 로지스틱 회귀로 와인 분류하기
```python
import pandas as pd
wine = pd.read_csv('https://bit.ly/wine-date')
wine.head()
```

![Untitled](/assets/HW1/bb1.png)

class에서 0이면 레드 와인, 1이면 화이트 와인(양성 클래스)이다. 즉, 와인 데이터에서 화이트 와인을 골라내는 문제.

**info()**메서드: 데이터프레임의 각 열의 데이터 타입과 누락된 데이터가 있는지 확인하는데 유용하다.

```python
wine.info()
```

![Untitled](/assets/HW1/bb2.png)

누락된 값이 있다면? 그 데이터를 버리거나 평균값으로 채운 후 사용할 수 있다. 

**describe()**메서드: 열에 대한 간략한 통계(최소, 최대, 평균값 등)를 출력해줌. 

```python
wine.describe()
```

![Untitled](/assets/HW1/bb3.png)

평균, 표준편차, 최소, 최대, 중간값, 1사분위수, 3사분위수를 알 수 있음. 

```python
#데이터 특성 표준화
data= wine[['alcohol','sugar','pH']].to_numpy()
target= wine['class'].to_numpy()

#훈련 세트와 테스트 세트로 나누기
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
```

```python
print(train_input.shape, test_input.shape)
```

>>(5197, 3) (1300, 3)

훈련세트 5197개, 테스트세트 1300개

```python
#훈련세트 전처리, 테스트세트 변환
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

>>0.7808350971714451
>>0.7776923076923077

훈련세트와 테스트 세트 점수가 모두 낮아 모델이 과소적합된 것을 알 수 있다. 

```python
print(lr.coef_, lr.intercept_)
```

>>[[ 0.51270274  1.6733911  -0.68767781]] [1.81777902]

학습 결과를 설명하기 어렵다..

### 결정 트리

**결정트리**: 여러 개의 결정 노트와 분기로 구성되며 이들을 통해 데이터를 특정 기준에 따라 반복적으로 나누어 최종적으로 예측 결과를 도출한다.

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier() #객체생성
dt.fit(train_scaled, train_target) #모델학습
print(dt.score(train_scaled, train_target)) #훈련 세트
print(dt.score(test_scaled, test_target)) #테스트 세트
```

>>0.996921300750433
>>0.8569230769230769

score() 메서드: 모델의 성능 평가. 정확도 계산

훈련세트에 대한 점수가 높다. 그에비해 테스트 세트의 성능은 낮다. 

plot_tree()함수 

```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```

![Untitled](/assets/HW1/bb4.png)

맨 위의 노트를 루트노트, 맨 아래 끝에 달린 노드를 리프노트 라고 한다. 

노드는 결정 트리를 구성하는 핵심 요소이다. 노드는 훈련 데이터의 특성에 대한 테스트를 표현한다. 

max_depth매개변수를 1로 주면 루트 노드를 제외하고 하나의 노드를 더 확장해 그린다. 

filled 매개변수에서 클래스에 맞게 노드의 색을 칠할 수 있다.

```python
plt.figure(figsize=(10.7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol','sugar','pH'])
plt.show()
```

![Untitled](/assets/HW1/bb5.png)

-0.239와 같거나 작으면 왼쪽 가지로, 그렇지 않으면 가지로 이동한다. 왼쪽이 Yes, 오른쪽이 No이다. 루트 노드의 촘 샘플수는 5197개이다. 이 중 음성클래스(레드 와인)는 1,258개이고, 양성클래스(화이트와인) 3,939개이다. 

plot_tree()함수에서 filled=True로 지정하면 클래스마다 색깔 부여해서 클래스 비율이 높아질수록 진한색으로 표시한다. 

결정 트리에서 리프 노드에서 가장 많은 클래스가 예측 클래스가 된다. 

### 불순도

DecisionTreeClassifier 클래스의 criterion 매개변수의 기본값이 gini이다. criterion 매개변수의 용도는 노드에서 데이터를 분할할 기준을 정하는 것이다. 

지니 불순도 = 1 - (음성 클래스 비율^2 +양성 클래스 비율^2)

결정 트리 모델은 부모 노트와 자식 노드의 불순도 차이가 가능한 크도록 트리를 성장시킨다. 

![Untitled](/assets/HW1/bb11.png)

이런 부모와 자식 노드 사이의 불순도 차이를 **정보이득** 이라고 부른다. 

DecisionTreeClassifier 클래스에서 criterion =’entropy’를 지정해 엔트로피 불순도를 사용할 수 있다. 엔트로피 불순도도 노드의 클래스 비율을 사용하지만 지니 불순도처럼 제곱이 아니라 밑이 2인 로그 사용해 곱한다. 

![Untitled](/assets/HW1/bb12.png)

### 가지치기

가지치기를 안하면 훈련세트에는 아주 잘 맞겠지만 테스트세트에서 점수는 그에 못 미칠것임. (일반화가 잘 안될것)

```python
#max_depth3지정, 루트 노트 아래 최대 3개의 노드까지만 성장할 수 있다.
dt =DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```

>>0.8454877814123533
>>0.8415384615384616

```python
plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol','sugar','pH'])
plt.show()
```

![Untitled](/assets/HW1/bb6.png)

특성값의 스케일이 계산에 영향을 미치지 않는다. 따라서 표준화 전처리를 할 필요가 없다.  

그럼 전처리 하기 전의 훈련세트, 테스트세트로 결정 트리 모델을 다시 훈련해보자.

```python
dt=DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)
print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))

```

>>0.8454877814123533
>>0.8415384615384616

```python
plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol','sugar','pH'])
plt.show()
```

![Untitled](/assets/HW1/bb7.png)

결과를 보면 같은 트리지만 특성값을 표준점수로 바꾸지 않은 터라 이해하기 훨씬 쉽다. 

마지막으로 결정 트리는 어떤 특성이 가장 유용한지 나타내는 특성 중요도를 계산해준다. 이 트리의 루트 노트와 깊이 1에서 당도를 사용해서 아마 당도가 가장 유용한 특성 중 하나일 것이다. 

```python
print(dt.feature_importances_)
```

>> [0.12345626 0.86862934 0.0079144 ]

확인하니 역시 두 번째 특성인 당도가 0.87정도로 특성 중요도가 가장 높다. 이를 모두 더하면 1이 된다. 

특성 중요도를 활용하면 결정 트리 모델의 특성 선택에 활용할 수 있다. 이것이 결정 트리 알고리즘의 또 다른 장점!

정리

- **결정트리:** 예/아니오에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘이다. 비교적 예측 과정을 이해하기 쉽고 성능도 뛰어난다.
- **불순도**: 결정 트리가 최적의 질문을 찾기 위한 기준이다. 사이킷런은 지니 불순도와 엔트로피 불순도를 제공한다.
- **정보 이득**: 부모 노드와 자식 노드의 불순도 차이이다. 결정 트리 알고리즘은 정보 이득이 최대화되도록 학습한다.
- 결정 트리는 제한 없이 성장하면 훈련 세트에 과대적합되기 쉽다. **가지치기**는 결정 트리의 성장을 제한하는 방법이다. 사이킷런의 결정 트리 알고리즘은 여러 가지 가지치기 매개변수를 제공한다.
- **특성 중요도**: 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값. 특성 중요도를 계산할 수 있는 것이 결정 트리의 또 다른 큰 장점.
