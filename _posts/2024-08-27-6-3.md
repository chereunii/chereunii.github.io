---
layout: post
title:  "06-3 주성분 분석"
date:   2024-08-27 
categories: [Khuda, week6]
---
### 차원과 차원 축소

차원을 줄이면 저장공간을 절약할 수 있다. 이를 위해 비지도 학습 작업중 하나인 **차원축소** 알고리즘을 다루어보자. 

대표적인 차원 축소 알고리즘인 **주성분 분석**을 공부해보자. 주성분 분석을 **PCA**라고도 부른다. 

### 주성분 분석 소개

주성분: 데이터의 변동성을 최대한 설명할 수 있는 방향을 나타내는 새로운 축이다. 주성분은 원본 차원과 같고 주성분으로 바꾼 데이터는 차원이 줄어든다. 

분산이 큰 방향을 찾고 이 벡터가 주성분임. 주성분 벡터의 원소 개수는 원본 데이터셋에 있는 특성 개수와 같다. 주성분은 원본 차원과 같고 주성분으로 바꾼 데이터는 차원이 줄어든다. 

첫 번째 주성분을 찾은 다음 이 벡터에 수직이고 분산이 가장 큰 다음 방향을 찾는다. 이 벡터가 두번째 주성분이다. 

### PCAS클래스

```python
!wget https://bit.ly/fruits_300 -0 fruits_300.npy_
import numpy as np
fruits =np.load('fruits_300.npy')
fruits_2d= fruits.reshape(-1,100*100)
```

```python
from sklearn.decomposition import PCA
pca= PCA(n_components=50)
pca.fit(fruits_2d)
print(pca.components_.shape)
```

>>(50, 10000)

n_components=50 으로 지정했기 때문에 pca.components_배열의 첫 번째 차원이 50이다. 즉 50 개의 주성분을 찾은 것이다. 두 번째 차원은 항상 원본 데이터 특성 개수와 같은 10,000이다.

```python
draw_fruits(pca.components_.reshape(-1,100,100))
```

![Untitled](/assets/HW1/ee1.png)

이 주성분은 원본 데이터에서 가장 분산이 큰 방향을 순서대로 나타낸 것이다. transform()메서드 사용해 원본 데이터 차원을 50으로 줄여보

```python
print(fruits_2d.shape)
```

>>(300, 10000)

```python
fruits_pca= pca.transform(fruits_2d)
print(fruits_pca.shape)
```

>>(300, 50)

fruits_2d는 (300, 10000)크기의 배열이었다. 50개의 주성분을 찾은 PCA모델을 사용해 이를 (300, 50)크기의 배열로 변환했다. 이제 fruits_pca배열은 50 개의 특성을 가진 데이터이다.

무려 1/200로 줄어들었다. 그럼 데이터의 차원을 줄였다면 다시 원상 복구할 수도 있을까?

### 원본 데이터 재구성

```python
fruits_inverse =pca.inverse_transform(fruits_pca)
print(fruits_inverse.shape)
```

>>(300, 10000)

10000개의 특성이 복원되었다.

```python
fruits_reconstruct= fruits_inverse.reshape(-1,100,100)
for start in [0,100, 200]:
  draw_fruits(fruits_reconstruct[start:start+100])
  print("\n")
```

![Untitled](/assets/HW1/ee2.png)

### 설명된 분산

주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값을 **설명된 분산**이라고 한다. PCA클래스의 explained_variance_ratio_에 각 주성분의 설명된 분산 비율이 기록되어있다.

```python
print(np.sum(pca.explained_variance_ratio_))
```

>>0.9215190262621741

92%가 넘는 분산을 유지하고 있다.

```python
plt.plot(pca.explained_variance_ratio_)

```

![Untitled](/assets/HW1/ee3.png)

그래프를 보면 처음 10개의 주성분 대부분의 분산을 표현한다. 그 다음부터는 각 주성분이 설명하고 있는 분산은 비교적 작다.

### 다른 알고리즘 함께 사용하기

```python
from sklearn.linear_model import LogisticRegression
lr= LogisticRegression()
target= np.array([0]*100+[1]*100+[2]*100)
```

사과를 0, 파인애플 1, 바나나 2로 지정하자.

```python
from sklearn.model_selection import cross_validate
scores= cross_validate(lr, fruits_2d,target)
print(np.mean(scores['test_score']),np.mean(scores['fit_time']))
```

![Untitled](/assets/HW1/ee4.png)

교차 검증의 점수는 0.997 정도로 매우 높다. 

```python
scores= cross_validate(lr, fruits_pca,target)
print(np.mean(scores['test_score']),np.mean(scores['fit_time']))
```

![Untitled](/assets/HW1/ee5.png)

```python
pca= PCA(n_components=0.5)
pca.fit(fruits_2d)
print(pca.n_components_)
```

>> 2

2개의 주성분을 찾았다.

```python
fruits_pca=pca.transform(fruits_2d)
print(fruits_pca.shape)
```

>>(300, 2)

```python
from sklearn.cluster import KMeans
km= KMeans(n_clusters=3,random_state=42)
km.fit(fruits_pca)
print(np.unique(km.labels_,return_counts=True))
```

![Untitled](/assets/HW1/ee6.png)

클러스터는 각각 91개, 99개, 110개의 샘플 포함하고 있다.

```python
for label in range(0,3):
  draw_fruits(fruits[km.labels_==label])
  print("\n")
```

![Untitled](/assets/HW1/ee7.png)
```python
for label in range(0,3):
  data= fruits_pca[km.labels_==label]
  plt.scatter(data[:,0],data[:,1])
plt.legend(['apple','pineapple','banana'])
plt.show()
```

![Untitled](/assets/HW1/ee8.png)
